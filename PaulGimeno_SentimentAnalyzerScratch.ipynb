{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PaulGimeno-SentimentAnalyzerScratch",
      "provenance": [],
      "mount_file_id": "1Ad8H-8wNkBWUMI7kUb-LWcUzTC8Ik7aS",
      "authorship_tag": "ABX9TyOOD23qi42nNs4StC/9/z02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gimenopea/EMSE6574/blob/master/PaulGimeno_SentimentAnalyzerScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Y8tKahyHB6"
      },
      "source": [
        "Task: build sentiment analyzer using no sentiment libraries other than native python classes and functions and pandas and matplotlib\n",
        "\n",
        "Review from https://www.yelp.com/biz/papa-johns-pizza-washington-10?osq=papa+john%27s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaOwXekvyodt"
      },
      "source": [
        "review_text = '''\n",
        "I always try to avoid delivery whether I'm in the Big City or in Cowtown because (1) I'm cheap and (2) you have to tip (I'm cheap) and (3) you are the mercy of delivery drivers and managers who can be great or be terrible. In my experience I always find myself eating sooner after I've ordered take-out, so that's a plus.\n",
        "SO: I've ordered carry-out twice from this Papa John's via their online ordering system (because it had special deals and coupon codes with dang good deals like a 3 or 5 topping pizza for $10). Both times they sent me an email that had a cool status thing on it (I realize this is commonplace now, but it was my first time with seeing the important uses we use our advanced tech for) that told me when my pizza was baking, boxed, and ready. Nice.\n",
        "I paid online after \"building my pizza\" under the deal. Said pizza was ready (approx. 20 minutes after ordering, maybe less) and I walked home. I really can't complain, except maybe about being the kind of person who would rather walk a mile home with two boxes from Papa John's rather than pay $X delivery fee (plus tip!).\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrxnL37_lk0L"
      },
      "source": [
        "# Clean non essential words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PWdYZJfylGH"
      },
      "source": [
        "def tokenizer(text, clean_non_chars = False):\n",
        "  split = text.split()\n",
        "  clean_split = []\n",
        "  if clean_non_chars:\n",
        "    for word in split:\n",
        "      word = ''.join( [letter for letter in word if word.isalpha() ])\n",
        "      clean_split.append(word) \n",
        "    return list(filter(lambda x: x.strip(), clean_split))\n",
        "  else:\n",
        "    return split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO_ABRh44fup"
      },
      "source": [
        "def freq_dist(text):\n",
        "  tokenized = tokenizer(text, clean_non_chars=True) \n",
        "  \n",
        "  word_table = {}\n",
        "  for word in tokenized:\n",
        "    if word in word_table:\n",
        "      word_table[word] += 1\n",
        "    else:\n",
        "      word_table[word] = 1\n",
        "\n",
        "  return word_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vQxI_hw4lmr"
      },
      "source": [
        "bag_of_words = freq_dist(review_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ULnYMuf7AZ3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xKQrRdI9B3T"
      },
      "source": [
        "sorted_bag_words = sorted(bag_of_words.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_sk-fE0BAo_"
      },
      "source": [
        "non_important_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WsO0x6JBPr_"
      },
      "source": [
        "def delete_non_important_words(dict_of_words):\n",
        "  clean_dict = {}\n",
        "  for k in dict_of_words:\n",
        "    if k not in non_important_words:\n",
        "      clean_dict[k] = dict_of_words[k]\n",
        "\n",
        "  return clean_dict\n",
        "\n",
        "clean_bag_of_words = delete_non_important_words(bag_of_words);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3GppUO3lqAE"
      },
      "source": [
        "# Count the clean word distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-uErwUclvYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7080b40-bcdb-4fe1-ccd8-4340e0563d04"
      },
      "source": [
        "len(clean_bag_of_words) \n",
        "# The number of essential words in this review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbpxsCvOlYS_"
      },
      "source": [
        "# Count positive words and/or adjectives in this review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71ZUbHdCYHN"
      },
      "source": [
        "list_of_positive_words = pd.read_csv('https://raw.githubusercontent.com/AmeyRuikar/sentiment-analysis/master/positive%20words.csv',header=None)\n",
        "list_of_positive_words.columns = ['index','word']\n",
        "list_of_positive_words.index = list_of_positive_words['word']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8tjjcU-Cgc8"
      },
      "source": [
        "positive_sentiment = {}\n",
        "for key,val in clean_bag_of_words.items():\n",
        "  if key in list_of_positive_words.index: \n",
        "    positive_sentiment[key] = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUDRK1iJjMTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e871c4-5f87-41a7-c9a3-f8e86b8f2382"
      },
      "source": [
        "#sum of positive sentiments\n",
        "\n",
        "positive_score = 0\n",
        "for val in positive_sentiment.values():\n",
        "  positive_score += 1\n",
        "print(positive_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMACnemwmI81"
      },
      "source": [
        "# Count negative words and/or adjectives in this review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFkTb4qmLcy"
      },
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "url = 'https://gist.githubusercontent.com/mkulakowski2/4289441/raw/dad8b64b307cd6df8068a379079becbb3f91101a/negative-words.txt'\n",
        "res = requests.get(url)\n",
        "a = res.text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyFBwAZpo4OU"
      },
      "source": [
        "a = a[a.index('\\n\\n2-faced'):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqyONKKVpGGG"
      },
      "source": [
        "neg_words = a.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWGPAfWIrq25"
      },
      "source": [
        "negative_sentiment = {}\n",
        "for key,val in clean_bag_of_words.items():\n",
        "  if key in neg_words: \n",
        "    negative_sentiment[key] = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdGqA8fUrzO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7a2c1f-5ed6-4203-eab2-4fecb45ec6ec"
      },
      "source": [
        "negative_sentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cheap': 1, 'commonplace': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVihsftBr1en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431c9770-653f-49a9-cc73-219b34a76e68"
      },
      "source": [
        "negative_score = 0\n",
        "for val in negative_sentiment.values():\n",
        "  negative_score += 1\n",
        "print(negative_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FqvxSgdtCPL"
      },
      "source": [
        "# is the review mostly positive or negative?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-F_fPrr-sG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e31cf5-7453-4efb-ac8a-80e07249f324"
      },
      "source": [
        "print('Positive word ratio {}%'.format(round(positive_score/len(clean_bag_of_words)*100),2))\n",
        "print('Negative word ratio {}%'.format(round(negative_score/len(clean_bag_of_words)*100),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive word ratio 11%\n",
            "Negative word ratio 3%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}